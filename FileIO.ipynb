{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FileIO.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MnMlD4sfgT2h"
      ],
      "authorship_tag": "ABX9TyOYZ9IcDRQmKryJowjWFBe5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddy0218/2022_ML_Earth_Env_Sci/blob/main/FileIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File I/O**\n",
        "In this notebook, we will introduce the basic functions we can use to store and retrieve data from files in different formats.\n",
        "\n",
        "For environmental sciences projects, research data are most commonly stored in the following formats: \n",
        "1.   Text files (TXT)\n",
        "2.   Tabular files (e.g., CSV, XLS)\n",
        "3.   Structured Data / Python dictionaries etc. (e.g., Pickle, dill, JSON)\n",
        "4.   Gridded data (e.g., HDF5, NetCDF)\n",
        "\n",
        "We will now see how we can use Python and different Python packages to retrieve the data stored in these formats, and how to save your data to different formats for future use.\n",
        "\n",
        "Reference: \n",
        "*   CUSP UCSL bootcamp 2017 (https://github.com/Mohitsharma44/ucsl17)\n",
        "*   Python 3 tutorial (https://docs.python.org/3/tutorial/inputoutput.html)\n",
        "*   GSFC Python Bootcamp (https://github.com/astg606/py_materials/blob/master/useful_modules/)\n",
        "*   Working on JSON Data in Python (https://realpython.com/python-json/)\n",
        "*   PyHOGS (http://pyhogs.github.io/intro_netcdf4.html)"
      ],
      "metadata": {
        "id": "pwD-d_vlhJfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import some packages first..."
      ],
      "metadata": {
        "id": "G2Yhd0OQ0NCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import netCDF4\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vpU1iJZRkmqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TXT files\n",
        "Now we will learn how to write stuff to a TXT file and read it back with built-in Python functions. The data used in this part of the tutorial will be very simple. In the next exercises, we will introduce commands in community packages that allows us to read and store more complex data.\n",
        "\n",
        "#### Opening Files: \n",
        "Files can be opened using python's built-in `open()` function. The function will create a file object for subsequent operations. Use the following syntax to read a TXT file: \\\\\n",
        "`fhandler = open(file_name, access mode, encoding)`\n",
        "\n",
        "- `file_name`: The file name that you would like to perform your I/O operations on.\n",
        "- `encoding`: Encoding scheme to use to convert the stream of bytes to text. (Standard=`utf-8`)\n",
        "- `access_mode`: The way in which a file is opened, available choice for this option include:\n",
        "\n",
        "|access_mode | Its Function|\n",
        "|:------|------------:|\n",
        "|r\t|Opens a file as read only|\n",
        "|rb\t|Opens a file as read only in binary format|\n",
        "|r+\t|Opens a file for reading and writing|\n",
        "|rb+\t|Opens a file for reading and writing in binary format|\n",
        "|w\t|Opens a file for writing only|\n",
        "|wb\t|Opens a file for writing only in binary format|\n",
        "|w+\t|Opens a file for both reading and writing|\n",
        "|wb+\t|Opens a file for writing and reading in binary format|\n",
        "|a\t|Opens a file for appending|\n",
        "|ab\t|Opens a file for appending in binary|\n",
        "|a+\t|Opens a file for appending and reading|\n",
        "|ab+\t|Opens a file for appending and reading in binary format|\n",
        "\n",
        "In the example below, we try to store several sentences into a new TXT file, and use the `open()` function to see if the code works as intended."
      ],
      "metadata": {
        "id": "lBq-bSDO0HOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRkLE8B3hHLi"
      },
      "outputs": [],
      "source": [
        "fhandler = open('test.txt', 'w', encoding=\"utf-8\") \n",
        "fhandler.write('Hello World!\\n')\n",
        "fhandler.write('I am a UNIL Master Student.\\n')\n",
        "fhandler.write('I am learning how to code!\\n')\n",
        "fhandler.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Comment:\n",
        "In the code above, we use the `open()` command to create a *write-only* (`access_mode='w'`) file `test.txt`. The open command creates a file object (`fhandler`) on which we can perform extra operations.\n",
        "\n",
        "We then try to add three sentences to the TXT file using the `.write()` operation on the file object.\n",
        "\n",
        "Remember to close the file with `.close()` command so that the changes can be finalized!\n",
        "\n",
        "If the code is writing, we should see a `test.txt` file created in the same path as this notebook. Let's see if that's the case!"
      ],
      "metadata": {
        "id": "OMsU4MGDB7aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfER3q0nB1O4",
        "outputId": "9f696d2d-b1b7-453f-fd1a-bf9e2b087fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cat test.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0n25-7gHg1R",
        "outputId": "5ad9c81c-8110-4992-e598-be53b0b0c223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "I am a UNIL Master Student.\n",
            "I am learning how to code!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hurray! It is working! 😀\n",
        "\n",
        "But didn't we just say we want to read it back? 🤨\n",
        "\n",
        "Let try to read the file then! Can you think of ways to do this?\n",
        "\n",
        "Here are some of the functions that you may end up using.\n",
        "\n",
        "1.   .close(): Close the file that we have currently open.\n",
        "2.   .readline([size]): Read strings from a file till it reaches a new line character `\\n` if the `size` parameter is empty. Otherwise it will read string of the given size.\n",
        "3.   .readlines([size]): Repeatly call `.readline()` till the end of the file.\n",
        "4.   .write(str): Writes the string str to file.\n",
        "5.   .writelines([list]): Write a sequence of strings to file. No new line is added automatically."
      ],
      "metadata": {
        "id": "KwdbjVJiEpX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fhandler = open('test.txt','r',encoding='utf-8')\n",
        "fhandler.readlines()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERZXsnTuFPPU",
        "outputId": "04010c86-01cb-4589-f4e8-c5bf0ff0fe77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello World!\\n',\n",
              " 'I am a UNIL Master Student.\\n',\n",
              " 'I am learning how to code!\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to add some text to the file?"
      ],
      "metadata": {
        "id": "TfXyXBX0UNwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test.txt', 'r+') as fhandler:\n",
        "  print(fhandler.readlines())\n",
        "  fhandler.writelines(['Now,\\n', 'I am trying to', ' add some stuff.'])\n",
        "  # Go to the starting of file\n",
        "  fhandler.seek(0)\n",
        "  # Print the content of file\n",
        "  print(fhandler.readlines())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCrMAMs7UJZM",
        "outputId": "ac22bfbb-47f7-4d6d-8ab9-9aaa03d07626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello World!\\n', 'I am a UNIL Master Student.\\n', 'I am learning how to code!\\n']\n",
            "['Hello World!\\n', 'I am a UNIL Master Student.\\n', 'I am learning how to code!\\n', 'Now,\\n', 'I am trying to add some stuff.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use an alternative way to open and write the data file. \n",
        "By using the `with` statement to open the TXT file, we ensure that the data is automatically closed after the final operation. We now do not need to write the `fhandler.close()` statement any more."
      ],
      "metadata": {
        "id": "LPY_mdCvVWuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise \n",
        "Let use what we have just learned to write a slightly more complex file.\n",
        "1.   Create a file called \"ans.txt\"\n",
        "2.   Write the following sentence \"This is my first I/O exercise.\"\n",
        "3.   Save the file\n",
        "4.   Read the file again, add 3 more new sentences: \"I just learned that pi is approximately 3.1416\", \"Writing this to the file,\", \"is a piece of cake!\"\n",
        "\n",
        "Instruction: I don't remember the exact pi value, but I want to change the accuracy whenever I want. So please import math and get the exact pi value. And change the pi value according to the desired accuracy."
      ],
      "metadata": {
        "id": "MnMlD4sfgT2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "with open(__, 'w', encoding=\"utf-8\") as fhandler:\n",
        "  fhandler.write('This is my first I/O exercise.\\n')\n",
        "\n",
        "with open(__, 'r+', encoding=\"utf-8\") as fhandler:\n",
        "  print(fhandler.readlines())\n",
        "  firstline = f'_____ {math.pi:.4f}__'\n",
        "  secondline = '_______'\n",
        "  thirdline = '____________'\n",
        "  fhandler.writelines([firstline,__,__])\n",
        "  # Go to the starting of file\n",
        "  fhandler.seek(0)\n",
        "  # Print the content of file\n",
        "  print(fhandler.readlines()) "
      ],
      "metadata": {
        "id": "iPwavPKMz_X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Answer"
      ],
      "metadata": {
        "id": "WPupgDID0Mrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "with open('ans.txt', 'w', encoding=\"utf-8\") as fhandler:\n",
        "  fhandler.write('This is my first I/O exercise.\\n')\n",
        "\n",
        "with open('ans.txt', 'r+', encoding=\"utf-8\") as fhandler:\n",
        "  print(fhandler.readlines())\n",
        "  firstline = f'I just learned that pi is approximately {math.pi:.4f}.\\n'\n",
        "  secondline = 'Writing this to the file,\\n'\n",
        "  thirdline = 'is a piece of cake!'\n",
        "  fhandler.writelines([firstline,secondline,thirdline])\n",
        "  # Go to the starting of file\n",
        "  fhandler.seek(0)\n",
        "  # Print the content of file\n",
        "  print(fhandler.readlines())  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RitVb6X-il3b",
        "outputId": "b7f7d934-b582-4f63-b928-5cde1fbd2906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is my first I/O exercise.\\n']\n",
            "['This is my first I/O exercise.\\n', 'I just learned that pi is approximately 3.1416.\\n', 'Writing this to the file,\\n', 'is a piece of cake!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular data\n",
        "What would you do if you have data that are nicely organized in the following format:\n",
        "```\n",
        "Data1, Data2, Data3\n",
        "Example01, Example02, Example03\n",
        "Example11, Example12, Example13\n",
        "```\n",
        "When you open a file that looks like this in Excel, this is how it would look like,\n",
        "\n",
        "||||\n",
        "|:--|:--|:--|\n",
        "|Data1\t|Data2\t|Data3|\n",
        "|Example1\t|Example2\t|Example3|\n",
        "\n",
        "This is a comma-separated tabular file. Files like these are commonly stored in CSV format. .CSV files can then be opened and viewed using a Spreadsheet program such as Google Sheets, Numbers or Microsoft Excel. \n",
        "\n",
        "But what if I want to use the data in Python?\n",
        "\n",
        "#### Opening Files: \n",
        "Luckily, there are community packages that could help you import and retrieve your tabular data with minimal effort. Here, we will introduce two such packages: CSV and Pandas.\n",
        "\n",
        "##### Reading CSV files with \"CSV\" package\n",
        "\n",
        "`reader()` can be used to create an object that is used to read the data from a csv file. The reader can be used as an iterator to process the rows of the file in order. Lets take a look at an example:"
      ],
      "metadata": {
        "id": "U-QNl3CHWyQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = []\n",
        "with open('./sample_data/ALO.csv', 'r') as fh:\n",
        "  reader = csv.reader(fh)\n",
        "  for info in reader:\n",
        "    row.append(info)"
      ],
      "metadata": {
        "id": "3LnaPBmnWbe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(row[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MHKhvnr_G9r",
        "outputId": "064d5461-4293-41a0-ca0e-2648c0f676f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['station', 'valid', 'tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'alti', 'mslp', 'vsby', 'gust', 'skyc1', 'skyc2', 'skyc3', 'skyc4', 'skyl1', 'skyl2', 'skyl3', 'skyl4', 'wxcodes', 'ice_accretion_1hr', 'ice_accretion_3hr', 'ice_accretion_6hr', 'peak_wind_gust', 'peak_wind_drct', 'peak_wind_time', 'feel', 'metar', 'snowdepth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(row[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOaKn3zc_Hp2",
        "outputId": "cf22bc2a-9cc2-4ad1-841c-0947982dbc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ALO', '2022-01-01 00:54', '26.10', '19.90', '77.06', '350.00', '14.00', '0.00', '29.80', '1010.30', '9.00', 'M', 'BKN', 'OVC', 'M', 'M', '1600.00', '2300.00', 'M', 'M', 'M', 'M', 'M', 'M', '26.00', '340.00', '2021-12-31 23:59', '13.56', 'KALO 010054Z 35014KT 9SM BKN016 OVC023 M03/M07 A2980 RMK AO2 PK WND 34026/2359 SLP103 T10331067 $', 'M']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Comment:\n",
        "In the code above, we use the `csv.reader()` method to read iteratively process each row in the CSV file. \n",
        "\n",
        "We add one new row to a empty list at each iteration. \n",
        "\n",
        "Using the `print()` function to look at what was written to the list. We found that the first row contains variable name information, whereas the second row contains data at a given time step.\n",
        "\n",
        "#### Extract data and write to new CSV file:\n",
        "The CSV file that we just imported actually contains weather station data from January 2022 to August 2022. What if we want data from the first five rows only? Can we extract the data and save it to a new CSV file?"
      ],
      "metadata": {
        "id": "kxbde1LL_gK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('testsmall.csv', 'w') as fh:\n",
        "  writer = csv.writer(fh)\n",
        "  for num in range(5):\n",
        "    writer.writerow(row[num])"
      ],
      "metadata": {
        "id": "phRXs1Uryjb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise \n",
        "In the demonstration we showed you how to extract 5 rows and write them to a new CSV file. The data rows are actually surface weather observations reconrded every hour. The data span from January to August. \n",
        "\n",
        "What if we want to extract January, February, and March data and save them in smaller files separated by month?\n",
        "\n",
        "Here we provide you with some functions to extract the indices of the first and final instance of the data on a given month in the tabular data. You don't need to change anything in these functions, in this exercise you just need to save the monthly files based on the output indices."
      ],
      "metadata": {
        "id": "iyRcScu00xIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_monthindices(month=None):\n",
        "  test = [rowobj[1].split(' ')[0].split('-')[1] for rowobj in row[1:]]\n",
        "  truefalse = []\n",
        "  for obj in test:\n",
        "    if obj==month:\n",
        "      truefalse.append(obj)\n",
        "    else:\n",
        "      truefalse.append(np.nan)\n",
        "  return pd.Series(truefalse).first_valid_index(),pd.Series(truefalse).last_valid_index()"
      ],
      "metadata": {
        "id": "8ghAiaUxA0kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Jan_index = output_monthindices(month='01')\n",
        "Feb_index = output_monthindices(month='02')\n",
        "Mar_index = output_monthindices(month='03')"
      ],
      "metadata": {
        "id": "INoE4nc1BhzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savefile = ['jan.csv','feb.csv','mar.csv']\n",
        "indices = [Jan_index, Feb_index, Mar_index]\n",
        "for i in range(3):\n",
        "  with open(savefile[i], 'w') as fh:\n",
        "    writer = csv.writer(fh)\n",
        "    for num in range(indices[i][0],indices[i][1]):\n",
        "      writer.writerow(row[num])"
      ],
      "metadata": {
        "id": "twNtp__rcCMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reading CSV files with Pandas\n",
        "Actually there is a better package for tabular data. The package is named 'Pandas'. We will introduce this package in greater details next week. For now, we will just demonstrate that we can use pandas to do the same FileI/O procedure we did earlier with CSV.\n",
        "\n",
        "Here, we read in the large weather station datasheet `ALO.csv` with pandas function `.read_csv()`."
      ],
      "metadata": {
        "id": "oJkjvLO9cmZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CSV file with pandas\n",
        "ALOdatasheet = pd.read_csv('./sample_data/ALO.csv') "
      ],
      "metadata": {
        "id": "s50nPbAIeMoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export first five rows in the Pandas dataframe to CSV file\n",
        "ALOdatasheet[0:5].to_csv('./testsmall_pd.csv')"
      ],
      "metadata": {
        "id": "2CmxJ1TqehDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serialization and Deserialization with Pickle\n",
        "(Rewritten from GSFC Python Bootcamp)\n",
        "\n",
        "The pickle is an internal Python format for writing arbitrary data to a file in a way that allows it to be read in again, intact.\n",
        "* `pickle` “serialises” the object first before writing it to file. \n",
        "* Pickling (serialization) is a way to convert a python object (list, dict, etc.) into a character stream which contains all the information necessary to reconstruct the object in another python script.\n",
        "\n",
        "The following types can be serialized and deserialized using the `pickle` module:\n",
        "* All native datatypes supported by Python (booleans, None, integers, floats, complex numbers, strings, bytes, byte arrays)\n",
        "* Dictionaries, sets, lists, and tuples - as long as they contain pickleable objects\n",
        "* Functions (pickled by their name references, and not by their value) and classes that are defined at the top level of a module.\n",
        "\n",
        "The main functions of `pickle` are:\n",
        "\n",
        "* `dump()`: pickles data by accepting data and a file object.\n",
        "* `load()`: takes a file object, reconstruct the objects from the pickled representation, and returns it.\n",
        "* `dumps()`: returns the pickled data as a string.\n",
        "* `loads()`: reads the pickled data from a string.\n",
        "\n",
        "`dump()`/`load()` serializes/deserializes objects through files but `dumps()`/`loads()` serializes/deserializes objects through string representation."
      ],
      "metadata": {
        "id": "O_S_sHCbfx6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Python dictionary\n",
        "data_org = { 'mydata1':np.linspace(0,800,801), 'mydata2':np.linspace(0,60,61)} "
      ],
      "metadata": {
        "id": "kkdg4VIGjBue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Python dictionary to pickle file\n",
        "with open('pickledict_sample.pkl', 'wb') as fid:\n",
        "     pickle.dump(data_org, fid)\n",
        "# Deserialize saved pickle file\n",
        "with open('pickledict_sample.pkl', 'rb') as fid:\n",
        "     data3 = pickle.load(fid)"
      ],
      "metadata": {
        "id": "CjWPpRfaj_kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for strg in data_org.keys():\n",
        "  print(f\"Variable {strg} is the same in data_org and data3: {(data_org[strg]==data3[strg]).all()}\")\n",
        "\n",
        "Jan_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M033JOoikcNj",
        "outputId": "48c3b081-9434-44a2-a182-f808ad725d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable mydata1 is the same in data_org and data3: True\n",
            "Variable mydata2 is the same in data_org and data3: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise \n",
        "Try to serialize one of the monthly surface weather station data we created in the CSV exercise, and read it back to see if we are getting the same data."
      ],
      "metadata": {
        "id": "wrb1w1GsmlMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__ = []\n",
        "for _ in range(_,_):\n",
        "    __.append(row[num])"
      ],
      "metadata": {
        "id": "Rb8b4AihmkbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(_, 'wb') as fid:\n",
        "     __(__, fid)\n",
        "with open(_, 'rb') as fid:\n",
        "     __ = __(fid)"
      ],
      "metadata": {
        "id": "OU1orXrFnN4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structral Data with JSON\n",
        "JSON is a popular format for structured data that can be used in Python, Perl among other languages.\n",
        "JSON format is built on a collection of name/value pairs. The name information can be an object, record, dictionary, hash table, keyed list, or associative array. The value paired with the name can be an array, vector, list, or sequence.\n",
        "\n",
        "We can use `json` package for I/O. The syntax of the package is very similar to `pickle`:\n",
        "\n",
        "* `dump()`: encoded string writing on file.\n",
        "* `load()`: Decode while JSON file read.\n",
        "* `dumps()`: encoding to JSON objects\n",
        "* `loads()`: Decode the JSON string.\n",
        "\n",
        "**Example of JSON Data**\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"stations\": [\n",
        "        {\n",
        "            \"acronym\": “BLD”, \n",
        "            \"name\": \"Boulder Colorado\",\n",
        "            \"latitude”: 40.00,\n",
        "            \"longitude”: -105.25\n",
        "        }, \n",
        "        {\n",
        "            \"acronym”: “BHD”, \n",
        "            \"name\": \"Baring Head Wellington New Zealand\",\n",
        "            \"latitude\": -41.28,\n",
        "            \"longitude\": 174.87\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "9kKvqrzLofiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to read this JSON dataframe with `json`!"
      ],
      "metadata": {
        "id": "_rp0S8wfA3oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json_data = '{\"stations\": [{\"acronym\": \"BLD\", \\\n",
        "                                \"name\": \"Boulder Colorado\", \\\n",
        "                            \"latitude\": 40.00, \\\n",
        "                            \"longitude\": -105.25}, \\\n",
        "                            {\"acronym\": \"BHD\", \\\n",
        "                             \"name\": \"Baring Head Wellington New Zealand\",\\\n",
        "                             \"latitude\": -41.28, \\\n",
        "                             \"longitude\": 174.87}]}'\n",
        "\n",
        "python_obj = json.loads(json_data)"
      ],
      "metadata": {
        "id": "9iDO86jt-Qvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in python_obj['stations']:\n",
        "    print(x[\"name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld6JHUla_8Yh",
        "outputId": "386a248a-7cbe-4841-bcb9-9d990abd863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boulder Colorado\n",
            "Baring Head Wellington New Zealand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert python_obj back to JSON\n",
        "print(json.dumps(python_obj, sort_keys=True, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBGyPft9BXPu",
        "outputId": "28a1af23-564b-47ca-be34-9b89a8b3a79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"stations\": [\n",
            "        {\n",
            "            \"acronym\": \"BLD\",\n",
            "            \"latitude\": 40.0,\n",
            "            \"longitude\": -105.25,\n",
            "            \"name\": \"Boulder Colorado\"\n",
            "        },\n",
            "        {\n",
            "            \"acronym\": \"BHD\",\n",
            "            \"latitude\": -41.28,\n",
            "            \"longitude\": 174.87,\n",
            "            \"name\": \"Baring Head Wellington New Zealand\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we try to convert a python object to JSON and write it to a file.\n",
        "Syntax for serialization and deserialization in the `json` package is almost the same as `pickle`"
      ],
      "metadata": {
        "id": "1kGZD0nMEcrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert python objects to JSON\n",
        "x = {\n",
        "  \"name\": \"John\",\n",
        "  \"age\": 30,\n",
        "  \"married\": True,\n",
        "  \"divorced\": False,\n",
        "  \"children\": (\"Ann\",\"Billy\"),\n",
        "  \"pets\": None,\n",
        "  \"cars\": [\n",
        "    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n",
        "    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "O-NyBmQHDexu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialization\n",
        "with open('./pythonobj.json','w') as sid:\n",
        "  json.dump(x,sid)\n",
        "# Deserialization\n",
        "with open('./pythonobj.json','r') as sid:\n",
        "  z = json.load(sid)\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omAjw1zXEWor",
        "outputId": "1dcb98f3-3489-4f0d-a3e1-566b5e1750bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'John', 'age': 30, 'married': True, 'divorced': False, 'children': ['Ann', 'Billy'], 'pets': None, 'cars': [{'model': 'BMW 230', 'mpg': 27.5}, {'model': 'Ford Edge', 'mpg': 24.1}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-dimensional gridded data with NetCDF4\n",
        "Geoscience datasets often contain multiple dimensions. For example, climate model outputs ususally contain 4 dimensions: time (t), vertical level (z), longitude (lon) and latitude (lat). These data are too complex to store in tabular tables. \n",
        "\n",
        "Developed at Unidata (subsidary of UCAR), the NetCDF format contains a hierarchial structure that allows better organization and storage of large multi-dimensional dataset, axes information, and other metadata. It is well suited to handle large numerical datasets as it allows users to access portions of a dataset without loading its entirety into memory.\n",
        "\n",
        "We can use `netCDF4` package to create, read and store data in NetCDF4. Another package `xarray` is available for this data format.\n",
        "\n",
        "#### **Here is how you would normally create and store data in a netCDF file:**\n",
        "\n",
        "\n",
        "1.   Open/create a netCDF dataset.\n",
        "2.   Define the dimensions of the data.\n",
        "3.   Construct netCDF variables using the defined dimensions.\n",
        "4.   Pass data into the netCDF variables.\n",
        "5.   Add attributes to the variables and dataset (optional but recommended).\n",
        "6.   Close the netCDF dataset."
      ],
      "metadata": {
        "id": "FkZWAHOEF2Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Open a netCDF4 dataset**"
      ],
      "metadata": {
        "id": "1sopHa-xMjiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ncfid = netCDF4.Dataset('sample_netcdf.nc', mode='w', format='NETCDF4')"
      ],
      "metadata": {
        "id": "AWMBC8alIU1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`modeType` has the options:\n",
        "* 'w': to create a new file\n",
        "* 'r+': to read and write with an existing file\n",
        "* 'r': to read (only) an existing file\n",
        "* 'a': to append to existing file\n",
        "\n",
        "`fileFormat` has the options: \n",
        "* 'NETCDF3_CLASSIC': Original netCDF format     \n",
        "* 'NETCDF3_64BIT_OFFSET': Used to ease the size restrictions of netCDF classic files\n",
        "* 'NETCDF4_CLASSIC'\n",
        "* 'NETCDF4': Offer new features such as groups, compound types, variable length arrays, new unsigned integer types, parallel I/O access, etc.\n",
        "* 'NETCDF3_64BIT_DATA'"
      ],
      "metadata": {
        "id": "PjsFKyJGKad-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Creating Dimensions in a netCDF File</font>**\n",
        "* Declare dimensions with `.createDimension(size)`\n",
        "* For unlimited dimensions, use `None` or `0` as size.\n",
        "* Unlimited size dimensions must be declared before (“to the left of”) other dimensions."
      ],
      "metadata": {
        "id": "iwGIzZLzLMcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data dimensions\n",
        "time = ncfid.createDimension('time', None)\n",
        "lev  = ncfid.createDimension('lev', 72)\n",
        "lat  = ncfid.createDimension('lat', 91)\n",
        "lon  = ncfid.createDimension('lon', 144)"
      ],
      "metadata": {
        "id": "_Pdul8bzJWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################################\n",
        "# Create dimension variables and data variable pre-filled with fill_value\n",
        "##########################################################################################\n",
        "# Dimension variables\n",
        "times      = ncfid.createVariable('time','f8',('time',))\n",
        "levels     = ncfid.createVariable('lev','i4',('lev',))\n",
        "latitudes  = ncfid.createVariable('lat','f4',('lat',))\n",
        "longitudes = ncfid.createVariable('lon','f4',('lon',))\n",
        "# Pre-filled data variable\n",
        "temp = ncfid.createVariable('temp','f4', \n",
        "                            ('time','lev','lat','lon',),\n",
        "                            fill_value=1.0e15)"
      ],
      "metadata": {
        "id": "X0NKo2qqMDqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Add variable attributes</font>**"
      ],
      "metadata": {
        "id": "kYXO70RoMqC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "latitudes.long_name  = 'latitude'\n",
        "latitudes.units      = 'degrees north'\n",
        "\n",
        "longitudes.long_name = 'longitude'\n",
        "longitudes.units     = 'degrees east'\n",
        "\n",
        "levels.long_name     = 'vertical levels'\n",
        "levels.units         = 'hPa'\n",
        "levels.positive      = 'down'\n",
        "\n",
        "beg_date = datetime.datetime(year=2019, month=1, day=1)\n",
        "times.long_name      = 'time'\n",
        "times.units          = beg_date.strftime('hours since %Y-%m-%d %H:%M:%S')\n",
        "times.calendar       = 'gregorian'\n",
        "\n",
        "temp.long_name       = 'temperature'\n",
        "temp.units           = 'K'\n",
        "temp.standard_name   = 'atmospheric_temperature'"
      ],
      "metadata": {
        "id": "8mpgpijRMzDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Write data on file**"
      ],
      "metadata": {
        "id": "FnDrRLnLNHkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latitudes[:]  =  np.arange(-90,91,2.0)\n",
        "longitudes[:] =  np.arange(-180,180,2.5)\n",
        "levels[:]     =  np.arange(0,72,1)\n",
        "\n",
        "out_frequency = 3   # ouput frequency in hours\n",
        "num_records   = 5\n",
        "dates = [beg_date + n*datetime.timedelta(hours=out_frequency) for n in range(num_records)]\n",
        "times[:] = netCDF4.date2num(dates, units=times.units, calendar=times.calendar)\n",
        "for i in range(num_records):\n",
        "    temp[i,:,:,:] = np.random.uniform(size=(levels.size,\n",
        "                                            latitudes.size,\n",
        "                                            longitudes.size))"
      ],
      "metadata": {
        "id": "E13tMUbJNGOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncfid.close()"
      ],
      "metadata": {
        "id": "C-BQABIUNUjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now we read the stored netCDF4 file to see what we did just now."
      ],
      "metadata": {
        "id": "IYQJNjKtNmpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "databank = netCDF4.Dataset('./sample_netcdf.nc', mode='r')"
      ],
      "metadata": {
        "id": "vbNBhuuPOMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We print the names of the variables in the `sample_netcdf.nc` file\n",
        "print(databank.variables.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzLZmoeGOUlQ",
        "outputId": "bf32483b-4a4e-4cf8-b5e3-cabe7d88e45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['time', 'lev', 'lat', 'lon', 'temp'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can read the data like this\n",
        "time   = ncfid.variables['time'][:]\n",
        "lev    = ncfid.variables['lev'][:]\n",
        "lat    = ncfid.variables['lat'][:]\n",
        "lon    = ncfid.variables['lon'][:]\n",
        "temp   = ncfid.variables['temp'][:]"
      ],
      "metadata": {
        "id": "5HXPmGQJNkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**\n",
        "\n",
        "While reading data from a file:\n",
        "\n",
        "- If you do not include `[:]` at the end of `variables[var_name]`, you are getting a variable object.\n",
        "- If you include `[:]` (or `[:,:]`, `[0, i:j, :]`, etc.) at the end of `variables[var_name]`, you are getting the Numpy array containing the data."
      ],
      "metadata": {
        "id": "dOvKUZyjO2F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8pszFS0O0-e",
        "outputId": "e96313e9-97d8-4597-f782-b59f8e264f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-90. -88. -86. -84. -82. -80. -78. -76. -74. -72. -70. -68. -66. -64.\n",
            " -62. -60. -58. -56. -54. -52. -50. -48. -46. -44. -42. -40. -38. -36.\n",
            " -34. -32. -30. -28. -26. -24. -22. -20. -18. -16. -14. -12. -10.  -8.\n",
            "  -6.  -4.  -2.   0.   2.   4.   6.   8.  10.  12.  14.  16.  18.  20.\n",
            "  22.  24.  26.  28.  30.  32.  34.  36.  38.  40.  42.  44.  46.  48.\n",
            "  50.  52.  54.  56.  58.  60.  62.  64.  66.  68.  70.  72.  74.  76.\n",
            "  78.  80.  82.  84.  86.  88.  90.]\n"
          ]
        }
      ]
    }
  ]
}